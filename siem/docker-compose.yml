services:
  # Services from M3_L7_Laboratorio_SIEM
  auth-service:
    build: ./auth-service
    ports:
      - '8000:8000'
    networks:
      - internal_net
    restart: unless-stopped

  user-service:
    build: ./user-service
    ports:
      - '8001:8001'
    networks:
      - internal_net
    restart: unless-stopped

  product-service:
    build: ./product-service
    ports:
      - '8002:8002'
    networks:
      - internal_net
    restart: unless-stopped

  kong:
    image: kong:3.5
    environment:
      - KONG_DATABASE=off
      - KONG_DECLARATIVE_CONFIG=/usr/local/kong/declarative/kong.yml
      - KONG_PORTAL_GUI_HOST=localhost:8003
      - KONG_PORT_MAPS=8443:443,8003:80
    volumes:
      - ./kong/kong.yml:/usr/local/kong/declarative/kong.yml
    ports:
      - '8003:8000'
      - '8443:8443'
      - '8004:8001'
    networks:
      - gateway_net
      - internal_net
    restart: unless-stopped

  opa:
    build: ./opa
    volumes:
      - ./opa/policies:/policies
    ports:
      - '9600:8181'
    networks:
      - gateway_net
    restart: unless-stopped

  filebeat:
    build: ./filebeat
    volumes:
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
    networks:
      - monitoring_net
    depends_on:
      - logstash
    restart: unless-stopped

  # Services from M8_L4_SIEM_zaproxy (newer versions)
  logstash:
    image: docker.elastic.co/logstash/logstash:8.13.4
    volumes:
      - ./logstash/pipeline:/usr/share/logstash/pipeline
    ports: ["5044:5044/udp"]
    networks:
      - monitoring_net
    depends_on: [elasticsearch]

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.13.4
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
    ports: ["9200:9200"]
    networks:
      - monitoring_net
    healthcheck: { test: ["CMD-SHELL", "curl -s http://localhost:9200 >/dev/null"], interval: 10s, retries: 30 }

  kibana:
    image: docker.elastic.co/kibana/kibana:8.13.4
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    ports: ["5601:5601"]
    networks:
      - monitoring_net
    depends_on: [elasticsearch]

  # Service from M3_L7_Laboratorio_SIEM
  netdata:
    image: netdata/netdata:latest
    container_name: netdata
    hostname: ${HOSTNAME}
    ports:
      - 19999:19999
    cap_add:
      - SYS_PTRACE
    security_opt:
      - apparmor:unconfined
    volumes:
      - netdataconfig:/etc/netdata
      - netdatalib:/var/lib/netdata
      - netdatacache:/var/cache/netdata
      - /etc/passwd:/host/etc/passwd:ro
      - /etc/group:/host/etc/group:ro
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /etc/os-release:/host/etc/os-release:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /var/lib/docker/:/var/lib/docker:ro
    environment:
      - NETDATA_CLAIM_TOKEN=ubeQs6JtYutnWZEHLvNkK5FSr1_b4BwNjUjk-fv3b4PVjv8asLUWDo3FmQTPfgyfkKy-TnGzd71bbjRi1xHsNKpKM9c9WGmbh0cqjHebonFjgPOSA2OMxy6UIyLq6kWEQQUVjdE
      - NETDATA_CLAIM_URL=https://app.netdata.cloud
      - NETDATA_CLAIM_ROOMS=d4f8a5da-8b1c-42ee-a08b-b4100ede7619

  # Services from M8_L5_Portafolio_SIEM_sonarqube (newer versions)
  prometheus:
    image: prom/prometheus
    container_name: pfm8_prometheus
    volumes:
      - ./monitoring/prometheus-grafana/prometheus.yml:/etc/prometheus/prometheus.yml:ro
    ports: ["9090:9090"]
    networks:
      - monitoring_net
    depends_on:
      - backend

  grafana:
    image: grafana/grafana
    container_name: pfm8_grafana
    environment:
      GF_SECURITY_ADMIN_USER: admin
      GF_SECURITY_ADMIN_PASSWORD: admin
    ports: ["3000:3000"]
    volumes:
      - ./monitoring/prometheus-grafana/grafana/provisioning:/etc/grafana/provisioning
      - ./monitoring/prometheus-grafana/grafana/dashboards:/var/lib/grafana/dashboards
    networks:
      - monitoring_net
    depends_on:
      - prometheus

  # Service from M8_L4_SIEM_zaproxy (newer version)
  jaeger:
    image: jaegertracing/all-in-one:1.57
    ports: ["16686:16686", "6831:6831/udp"]
    networks:
      - monitoring_net

  # Service from M3_L7_Laboratorio_SIEM
  cadvisor:
    image: gcr.io/cadvisor/cadvisor:latest
    ports:
      - '8080:8080'
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
    networks:
      - monitoring_net
    restart: unless-stopped

  # Service from M3_L7_Laboratorio_SIEM
  falco:
    image: falcosecurity/falco:latest
    privileged: true
    volumes:
      - /var/run/docker.sock:/host/var/run/docker.sock
      - /dev:/host/dev
      - /proc:/host/proc:ro
      - /boot:/host/boot:ro
      - /lib/modules:/host/lib/modules:ro
      - /usr:/host/usr:ro
      - ./falco/falco_rules.local.yaml:/etc/falco/falco_rules.local.yaml:ro
    networks:
      - monitoring_net
    restart: unless-stopped

  # Service from M3_L7_Laboratorio_SIEM
  suricata:
    image: jasonish/suricata:latest
    network_mode: host
    volumes:
      - ./suricata:/etc/suricata
    cap_add:
      - NET_ADMIN
      - NET_RAW
    restart: unless-stopped

  # Service from M8_L4_SIEM_zaproxy
  zap-af:
    image: ghcr.io/zaproxy/zaproxy:stable
    depends_on: [api]
    working_dir: /zap/wrk
    volumes:
      - ./zap-reports:/zap/wrk
    command: ["zap.sh","-cmd","-autorun","/zap/wrk/plan.yml"]

  # Service from M8_L5_Portafolio_SIEM_sonarqube
  sonarqube:
    image: sonarqube:community
    container_name: pfm8_sonarqube
    ports: ["9000:9000"]
    environment:
      SONAR_ES_BOOTSTRAP_CHECKS_DISABLE: "true"
    volumes:
      - sonar_data:/opt/sonarqube/data
      - sonar_logs:/opt/sonarqube/logs

  wazuh-indexer:
    image: wazuh/wazuh-indexer:4.7.5
    ports:
      - "9201:9200"
    environment:
      - "INDEXER_USERNAME=admin"
      - "INDEXER_PASSWORD=admin"
    volumes:
      - wazuh_indexer_data:/usr/share/wazuh-indexer/data

  wazuh-manager:
    image: wazuh/wazuh-manager:4.7.5
    ports:
      - "1514:1514"
      - "1515:1515"
      - "55000:55000"
    environment:
      - "WAZUH_INDEXER_URL=https://wazuh-indexer:9200"
      - "WAZUH_API_USERNAME=admin"
      - "WAZUH_API_PASSWORD=admin"
    volumes:
      - wazuh_manager_data:/var/ossec/data
    depends_on:
      - wazuh-indexer

  wazuh-dashboard:
    image: wazuh/wazuh-dashboard:4.7.5
    ports:
      - "5602:5601"
    environment:
      - "WAZUH_INDEXER_URL=https://wazuh-indexer:9200"
      - "WAZUH_API_URL=https://wazuh-manager:55000"
    depends_on:
      - wazuh-manager
    networks:
      - monitoring_net

  # Service from M8_L4_SIEM_zaproxy
  api:
    build: ./api
    env_file:
      - ./api/.env
    environment:
      - PYTHONUNBUFFERED=1
    ports:
      - "8000:8000"
    depends_on:
      - jaeger
      - logstash

  # Service from M8_L5_Portafolio_SIEM_sonarqube
  backend:
    build: ./backend
    container_name: pfm8_backend
    depends_on:
      db:
        condition: service_healthy
    environment:
      DATABASE_URL: postgresql+psycopg2://app:app@db:5432/appdb
      APP_ENV: "dev"
      CORS_ALLOW_ORIGINS: "*" 
      JWT_SECRET: "dev-secret"
    ports: ["8001:8000"] # Port changed to avoid conflict with api service
    volumes:
      - ./backend/app:/app/app
    command: >
      sh -c "uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload"
    healthcheck:
      test: ["CMD", "wget", "-qO-", "http://localhost:8000/health"]
      interval: 10s
      timeout: 5s
      retries: 20

  # Service from M8_L5_Portafolio_SIEM_sonarqube
  db:
    image: postgres:16
    container_name: pfm8_db
    environment:
      POSTGRES_USER: app
      POSTGRES_PASSWORD: app
      POSTGRES_DB: appdb
    ports: ["5432:5432"]
    volumes:
      - ./db/init.sql:/docker-entrypoint-initdb.d/00-init.sql:ro
      - db_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U app -d appdb"]
      interval: 10s
      timeout: 5s
      retries: 10

volumes:
  data01:
  netdataconfig:
  netdatalib:
  netdatacache:
  db_data:
  sonar_data:
  sonar_logs:
  wazuh_indexer_data:
  wazuh_manager_data:

networks:
  internal_net:
  monitoring_net:
  gateway_net:
  elastic:
  default:
    name: pfm8_net
    driver: bridge
